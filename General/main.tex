

\documentclass{resume}
\usepackage[left=0.5in,top=0.5in,right=0.5in,bottom=0.5in]{geometry}
\newcommand{\tab}[1]{\hspace{.2667\textwidth}\rlap{#1}}
\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\usepackage{hyperref}
\hypersetup{hidelinks}
\usepackage[normalem]{ulem}
\newcommand{\uhref}[2]{\href{#1}{\uline{#2}}}
\usepackage{enumitem}
\name{Kausik Amancherla}
\address{\textnormal{Seattle, WA} \addressSep \uhref{mailto:ucangetkausik@gmail.com}{ucangetkausik@gmail.com} \addressSep \uhref{https://www.linkedin.com/in/kausik-a/}{linkedin.com/in/kausik-a} \addressSep \uhref{https://kausik.bearblog.dev}{kausik.bearblog.dev}}
\begin{document}
\vspace{2pt}
\begin{rSection}{Summary}
  \small
  Software engineer with 4+ years at Amazon Web Services building high-throughput observability and developer-platform systems across ADOT/OpenTelemetry.
  Scaled ingestion to 1B+ metrics/sec at 99.8\% SLA; open-source developer experienced in Python/TypeScript/Go, distributed systems, CI/CD reliability, and agentic workflows.
\end{rSection}
\vspace{0pt}
\begin{rSection}{Technical Skills}
  \small
  \textbf{Languages:} Python, Go, Java, TypeScript/JavaScript, SQL, Kotlin \\
  \textbf{Core SWE:} Distributed systems, microservices, system design, REST APIs, gRPC, CI/CD pipelines, performance optimization, load testing, code reviews, Agile/Scrum \\
  \textbf{AI/ML \& Developer Tools:} LLM applications (local + hosted), PyTorch, scikit-learn, MLX; agentic coding tools (Claude Code, Codex, OpenCode); Model Context Protocol (MCP), function calling, multi-agent orchestration, RAG pipelines, prompt engineering, guardrails, and evaluation/tracing \\
  \textbf{Cloud, Observability \& Data:} AWS (Lambda, EKS/ECS, CloudFormation), Terraform, Docker, Kubernetes, OpenTelemetry, Prometheus, Grafana, GitHub Actions, MySQL, PostgreSQL, MongoDB, Kafka, Spark
\end{rSection}
\vspace{0pt}
\begin{rSection}{Experience}

  \normalsize
  {\bf Software Development Engineer at Amazon Web Services, Seattle WA} \hfill {Feb. 2022 -- Present}
  \begin{itemize}[itemsep=-7pt,topsep=-5pt]
    \item Shipped core AWS Distro for OpenTelemetry (ADOT) components for collector pipelines and Go/Java/Python instrumentation, improving telemetry collection efficiency by 24\% and expanding coverage across 3 language stacks.
    \item Scaled an internal OpenTelemetry Protocol (OTLP) ingestion service to sustain 1B+ metrics/sec aggregate peak throughput across the collector fleet while maintaining a 99.8\% SLA.
    \item Tuned reliability controls (batching, queue/retry, memory guardrails, rollout gates) to reduce telemetry drop/failure events by 33\% during peak traffic windows.
    \item Led rollout reliability strategy by building CI/CD pipeline validation harnesses and canary release checks (GitHub Actions, Terraform, CloudFormation), reducing failed rollout incidents by 41\% and rollback time by 36\%.
    \item Drove cross-functional system design of ambiguous observability asks into measurable experiments tied to p95 latency, error-budget, and telemetry-loss thresholds, increasing experiment-to-rollout success rate by 31\%.
    \item Partnered with enterprise customers to triage instrumentation regressions and production incidents across distributed microservices, reducing mean time to resolution by 29\%.
  \end{itemize}

  \normalsize
  {\bf Research Assistant at MORSE Studio Research Lab, Atlanta GA} \hfill {May 2021 -- Dec. 2021}
  \begin{itemize}[itemsep=-7pt,topsep=-5pt]
    \item Designed and deployed an underground radon-monitoring testbed with CS, Geoscience, and Physics researchers using Raspberry Pi nodes, Airthings radon monitors, and moisture/temperature sensors at 1 ft, 2 ft, and 3 ft depths; produced 8{,}331 timestamped observations over a 6-day field study.
    \item Implemented and compared direct/indirect radon diffusion length (RDL) models from time-series data, generating field-validated mitigation guidance (47.1--62.4 cm barrier thickness) for radon-resistant foundation design.
  \end{itemize}

  \normalsize
  {\bf Research Associate, Defence Research and Development Organisation, India} \hfill {May 2019 -- Jul 2019}
  \begin{itemize}[itemsep=-7pt,topsep=-5pt]
    \item Built an on-prem facial recognition pipeline for employee verification and intruder detection using Python, OpenCV, TensorFlow/Keras (FaceNet transfer learning), and Multi-task Cascaded Convolutional Networks (MTCNN) for face detection/alignment; achieved 91\% validation accuracy on internal test sets.
    \item Calibrated FaceNet FAR/FRR thresholds with augmentation and hard-negative mining, reducing false accepts by 26\% on low-light/occlusion validation sets.
    \item Deployed air-gapped, zero-egress inference workflows for classified internal environments.
  \end{itemize}


  \end{rSection}
\vspace{0pt}
\begin{rSection}{Education}
  \small
  {\bf Georgia State University, Atlanta, GA:} Master of Science in Computer Science \hfill {Dec. 2021}\\
  {\bf Gandhi Institute of Technology and Management, India:} Bachelor of Technology in Computer Science \hfill {May 2020}
\end{rSection}
\vspace{0pt}
\newpage
\begin{rSection}{Selected Projects}
  \small
  {\bf Thera-MLX (JAX/Flax to MLX Port)} \hfill {\sl Python, MLX, safetensors}
  \begin{itemize}[itemsep=-7pt,topsep=-3pt]
    \item Ported an arbitrary-scale super-resolution model from JAX/Flax to Apple MLX, including checkpoint conversion and full parameter-key parity validation.
    \item Implemented chunked decoding, encoder/source tiling, and profiling controls to run 4K inference on 16GB unified memory while lowering peak memory by 34\%.
  \end{itemize}

  {\bf Twitch VOD Summarizer} \hfill {\sl Python, mlx-whisper, pyannote, Gemini API}
  \begin{itemize}[itemsep=-7pt,topsep=-3pt]
    \item Built an end-to-end pipeline to download Twitch VODs, run speaker-aware transcription on Apple Silicon, and generate structured AI summaries.
    \item Automated download \(\rightarrow\) transcription \(\rightarrow\) diarization \(\rightarrow\) summarization flow, reducing manual analysis time per VOD by 76\%.
  \end{itemize}

  {\bf Email Proof-of-Work Platform} \hfill {\sl JavaScript, Bun, WebGPU, SSE}
  \begin{itemize}[itemsep=-7pt,topsep=-3pt]
    \item Shipped an end-to-end proof-generation and verification app with browser CPU/GPU workers, server-side streaming progress, signed payloads, and automated tests.
    \item Implemented auto backend selection (CPU/GPU) and streaming telemetry for mining sessions, improving proof-generation throughput by 43\% under benchmark workloads.
  \end{itemize}
\end{rSection}
\end{document}
